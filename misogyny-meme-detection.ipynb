{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11184551,"sourceType":"datasetVersion","datasetId":6981650}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load the train and dev CSV files\ntrain_df = pd.read_csv('/kaggle/input/mesogyny-train-dev-test/train/train.csv')\ndev_df = pd.read_csv('/kaggle/input/mesogyny-train-dev-test/dev/dev.csv')\ntest_df = pd.read_csv('/kaggle/input/mesogyny-train-dev-test/test/test.csv')\n# Check the first few rows of the train data\nprint(\"TRAIN DATA\\n\\n\",train_df.head())\nprint(\"DEV DATA\\n\\n\", dev_df.head())\nprint(\"TEST DATA\\n\\n\", test_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:12.285087Z","iopub.execute_input":"2025-04-01T10:32:12.285454Z","iopub.status.idle":"2025-04-01T10:32:12.318954Z","shell.execute_reply.started":"2025-04-01T10:32:12.285428Z","shell.execute_reply":"2025-04-01T10:32:12.317856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Assume 'train.csv' has columns 'image_name' and 'label'\ntrain_df['image_path'] = train_df['image_name'].apply(lambda x: f'/kaggle/input/mesogyny-train-dev-test/train/train images{x}')\n\n# Now you can access image paths and labels together\nprint(\"TRAIN DATA IMAGES\\n\", train_df[['image_name', 'image_path', 'labels']].head())\n\n\n# Example: Assume 'train.csv' has columns 'image_name' and 'label'\ndev_df['image_path'] = dev_df['image_name'].apply(lambda x: f'/kaggle/input/mesogyny-train-dev-test/dev/dev images{x}')\n\n# Now you can access image paths and labels together\nprint(\"DEV DATA IMAGES\\n\", dev_df[['image_name', 'image_path', 'labels']].head())\n\n\n# Example: Assume 'train.csv' has columns 'image_name' and 'label'\n# Example: Assume 'test.csv' has columns 'image_name'\ntest_df['image_path'] = test_df['image_name'].apply(lambda x: f'/kaggle/input/mesogyny-train-dev-test/test/test images{x}')\n\n# Now you can access image paths together (note: test data doesn't have 'labels' column)\nprint(\"TEST DATA IMAGES\\n\", test_df[['image_name', 'image_path']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:12.320375Z","iopub.execute_input":"2025-04-01T10:32:12.320675Z","iopub.status.idle":"2025-04-01T10:32:12.339925Z","shell.execute_reply.started":"2025-04-01T10:32:12.320653Z","shell.execute_reply":"2025-04-01T10:32:12.338363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install tesseract-ocr-chi-sim\n!pip install transformers torch torchvision efficientnet_pytorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:12.342243Z","iopub.execute_input":"2025-04-01T10:32:12.342616Z","iopub.status.idle":"2025-04-01T10:32:18.480525Z","shell.execute_reply.started":"2025-04-01T10:32:12.342582Z","shell.execute_reply":"2025-04-01T10:32:18.479602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to clean text (remove URLs, special characters, etc.)\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    # Remove special characters, numbers, and extra spaces\n    text = re.sub(r'[^a-zA-Z\\u4e00-\\u9fa5\\s]', '', text)  # Keep Chinese characters and spaces\n    text = text.strip()\n    return text\n\n# Apply cleaning to the transcription column in both train and dev dataframes\ntrain_df['cleaned_text'] = train_df['transcriptions'].apply(clean_text) \ndev_df['cleaned_text'] = dev_df['transcriptions'].apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.482040Z","iopub.execute_input":"2025-04-01T10:32:18.482327Z","iopub.status.idle":"2025-04-01T10:32:18.493603Z","shell.execute_reply.started":"2025-04-01T10:32:18.482302Z","shell.execute_reply":"2025-04-01T10:32:18.492626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Initialize the tokenizer for Chinese BERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n\n# Tokenize the cleaned text\ndef tokenize_text(text):\n    return tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n\n# Apply tokenization to the cleaned text\ntrain_df['tokenized'] = train_df['cleaned_text'].apply(lambda x: tokenize_text(x))\ndev_df['tokenized'] = dev_df['cleaned_text'].apply(lambda x: tokenize_text(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.494591Z","iopub.execute_input":"2025-04-01T10:32:18.494932Z","iopub.status.idle":"2025-04-01T10:32:18.851280Z","shell.execute_reply.started":"2025-04-01T10:32:18.494899Z","shell.execute_reply":"2025-04-01T10:32:18.850574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\n\n# Define image augmentation pipeline\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224 (standard input size for VGG)\n    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n    transforms.RandomRotation(10),  # Randomly rotate images within the range of [-10, 10] degrees\n    transforms.ToTensor(),  # Convert image to tensor\n])\n\n# Apply image preprocessing inside the dataset (later when loading)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.851987Z","iopub.execute_input":"2025-04-01T10:32:18.852200Z","iopub.status.idle":"2025-04-01T10:32:18.856986Z","shell.execute_reply.started":"2025-04-01T10:32:18.852175Z","shell.execute_reply":"2025-04-01T10:32:18.856005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map the string labels to numeric values (1 for Misogyny, 0 for Not-Misogyny)\ntrain_df['labels'] = train_df['labels'].map({'Misogyny': 1, 'Not-Misogyny': 0})\ndev_df['labels'] = dev_df['labels'].map({'Misogyny': 1, 'Not-Misogyny': 0})\n\n# Check for any NaN values in the 'labels' column after mapping\nprint(\"NaN Labels in Train Data:\", train_df['labels'].isna().sum())\nprint(\"NaN Labels in Dev Data:\", dev_df['labels'].isna().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.857733Z","iopub.execute_input":"2025-04-01T10:32:18.858033Z","iopub.status.idle":"2025-04-01T10:32:18.875400Z","shell.execute_reply.started":"2025-04-01T10:32:18.857999Z","shell.execute_reply":"2025-04-01T10:32:18.874410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if the labels are now correctly mapped\nprint(\"Train Data after Fixing Labels:\")\nprint(train_df[['image_name', 'cleaned_text', 'labels']].head())\n\nprint(\"Dev Data after Fixing Labels:\")\nprint(dev_df[['image_name', 'cleaned_text', 'labels']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.877780Z","iopub.execute_input":"2025-04-01T10:32:18.878004Z","iopub.status.idle":"2025-04-01T10:32:18.893643Z","shell.execute_reply.started":"2025-04-01T10:32:18.877986Z","shell.execute_reply":"2025-04-01T10:32:18.892815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\nfrom PIL import Image\nimport os\n\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, transform=None, device='cpu'):\n        self.data = dataframe.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.device = device  # 'cpu' or 'cuda'\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Load and preprocess image\n        img_path = os.path.join(self.image_dir, row['image_name'])\n        \n        # Check if image exists\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"Image {img_path} not found.\")\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize text\n        transcription = str(row['transcriptions'])\n        text_inputs = self.tokenizer(\n            transcription,\n            truncation=True,\n            padding='max_length',\n            max_length=64,\n            return_tensors=\"pt\"\n        )\n        \n        # Squeeze and move tensors to the specified device (e.g., CPU or GPU)\n        input_ids = text_inputs['input_ids'].squeeze(0).to(self.device)\n        attention_mask = text_inputs['attention_mask'].squeeze(0).to(self.device)\n\n        # Label encoding (1 for Misogyny, 0 for Not-Misogyny)\n        label = torch.tensor(row['labels'], dtype=torch.float).to(self.device)\n        \n        return {\n            'image': image,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': label\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:32:18.895073Z","iopub.execute_input":"2025-04-01T10:32:18.895384Z","iopub.status.idle":"2025-04-01T10:32:18.907391Z","shell.execute_reply.started":"2025-04-01T10:32:18.895351Z","shell.execute_reply":"2025-04-01T10:32:18.906589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\nfrom transformers import AutoModel\nimport torch\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, dropout=0.3):\n        super(MultimodalClassifier, self).__init__()\n\n        # Image model (VGG16)\n        self.vgg_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        self.vgg_model.classifier = self.vgg_model.classifier[:3]  # Remove final classifier\n\n        # Text model (Chinese BERT)\n        self.text_model = AutoModel.from_pretrained(\"bert-base-chinese\")\n\n        # Fusion Layer: Concatenate image and text features, then pass through fully connected layers\n        self.fc = nn.Sequential(\n            nn.Linear(4096 + 768, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)  # Binary classification\n        )\n\n    def forward(self, image, input_ids, attention_mask):\n        # Extract image features\n        image_feat = self.vgg_model(image)\n\n        # Extract text features using pooled output (CLS token embedding)\n        text_feat = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n\n        # Concatenate image and text features\n        combined = torch.cat((image_feat, text_feat), dim=1)\n\n        # Pass through the fully connected fusion layer\n        output = self.fc(combined)\n        return output  # BCEWithLogitsLoss expects raw logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:41:04.525681Z","iopub.execute_input":"2025-04-01T10:41:04.526057Z","iopub.status.idle":"2025-04-01T10:41:04.532654Z","shell.execute_reply.started":"2025-04-01T10:41:04.526030Z","shell.execute_reply":"2025-04-01T10:41:04.531698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Prepare the train dataset and DataLoader\ntrain_dataset = MemeDataset(\n    dataframe=train_df,\n    image_dir=\"/kaggle/input/mesogyny-train-dev-test/train/train images\",  # Use the correct path to the train images\n    tokenizer=tokenizer,\n    transform=image_transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Prepare the dev dataset and DataLoader\ndev_dataset = MemeDataset(\n    dataframe=dev_df,\n    image_dir=\"/kaggle/input/mesogyny-train-dev-test/dev/dev images\",  # Use the correct path to the dev images\n    tokenizer=tokenizer,\n    transform=image_transform\n)\n\ndev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:41:11.846670Z","iopub.execute_input":"2025-04-01T10:41:11.846983Z","iopub.status.idle":"2025-04-01T10:41:11.853510Z","shell.execute_reply.started":"2025-04-01T10:41:11.846961Z","shell.execute_reply":"2025-04-01T10:41:11.852559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cpu'  # or \"cuda\" if available\nmodel = MultimodalClassifier().to(device)\n\n# Just a quick forward pass to verify everything is working\nfor batch in train_loader:\n    images = batch['image'].to(device)\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n\n    output = model(images, input_ids, attention_mask)\n    print(\"Output shape:\", output.shape)  # Should be [batch_size, 1]\n    break  # Only test the first batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:41:13.789003Z","iopub.execute_input":"2025-04-01T10:41:13.789319Z","iopub.status.idle":"2025-04-01T10:41:25.957233Z","shell.execute_reply.started":"2025-04-01T10:41:13.789293Z","shell.execute_reply":"2025-04-01T10:41:25.956429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\n# Define the loss function (BCEWithLogitsLoss for binary classification)\ncriterion = nn.BCEWithLogitsLoss()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:41:25.958426Z","iopub.execute_input":"2025-04-01T10:41:25.958707Z","iopub.status.idle":"2025-04-01T10:41:25.962371Z","shell.execute_reply.started":"2025-04-01T10:41:25.958686Z","shell.execute_reply":"2025-04-01T10:41:25.961619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom transformers import get_cosine_schedule_with_warmup\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model\nmodel = MultimodalClassifier().to(device)\n\n# Use BCEWithLogitsLoss\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-5)  # Smaller LR\n\n# Scheduler\nepochs = 8\ntotal_steps = len(train_loader) * epochs\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1 * total_steps),\n    num_training_steps=total_steps\n)\n\n# Training function\ndef train_epoch(model, train_loader, optimizer, scheduler, criterion, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in train_loader:\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device).unsqueeze(1)\n\n        optimizer.zero_grad()\n        logits = model(images, input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n        all_preds.extend(preds.astype(int).flatten())\n        all_labels.extend(labels.cpu().numpy().flatten())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    prec = precision_score(all_labels, all_preds)\n    rec = recall_score(all_labels, all_preds)\n\n    return total_loss / len(train_loader), acc, f1, prec, rec\n\n# Evaluation function\ndef evaluate(model, dev_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in dev_loader:\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device).unsqueeze(1)  # FIXED HERE\n\n            logits = model(images, input_ids, attention_mask)\n            loss = criterion(logits, labels)\n            total_loss += loss.item()\n\n            preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n            all_preds.extend(preds.astype(int).flatten())\n            all_labels.extend(labels.cpu().numpy().flatten())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    prec = precision_score(all_labels, all_preds)\n    rec = recall_score(all_labels, all_preds)\n\n    return total_loss / len(dev_loader), acc, f1, prec, rec\n\n\n\n# Training loop\nfor epoch in range(epochs):\n    train_loss, train_acc, train_f1, train_prec, train_rec = train_epoch(\n        model, train_loader, optimizer, scheduler, criterion, device\n    )\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    print(f\"Train → Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Macro F1: {train_f1:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f}\")\n\n    val_loss, val_acc, val_f1, val_prec, val_rec = evaluate(\n        model, dev_loader, criterion, device\n    ) \n    print(f\"Val   → Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro F1: {val_f1:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:41:25.963918Z","iopub.execute_input":"2025-04-01T10:41:25.964212Z","iopub.status.idle":"2025-04-01T10:48:35.788412Z","shell.execute_reply.started":"2025-04-01T10:41:25.964191Z","shell.execute_reply":"2025-04-01T10:48:35.787689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, transform=None, device='cpu', is_test=False):\n        self.data = dataframe.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.device = device  # 'cpu' or 'cuda'\n        self.is_test = is_test  # Flag to identify test dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Load and preprocess image\n        img_path = os.path.join(self.image_dir, row['image_name'])\n        \n        # Check if image exists\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"Image {img_path} not found.\")\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize text\n        transcription = str(row['transcriptions'])\n        text_inputs = self.tokenizer(\n            transcription,\n            truncation=True,\n            padding='max_length',\n            max_length=64,\n            return_tensors=\"pt\"\n        )\n        \n        # Squeeze and move tensors to the specified device (e.g., CPU or GPU)\n        input_ids = text_inputs['input_ids'].squeeze(0).to(self.device)\n        attention_mask = text_inputs['attention_mask'].squeeze(0).to(self.device)\n\n        # If it's not a test dataset, add labels; else, return only text and image\n        if not self.is_test:\n            label = torch.tensor(row['labels'], dtype=torch.float).to(self.device)\n            return {\n                'image': image,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'label': label\n            }\n        else:\n            return {\n                'image': image,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask\n            }\n            \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:48:43.719934Z","iopub.execute_input":"2025-04-01T10:48:43.720245Z","iopub.status.idle":"2025-04-01T10:48:43.727348Z","shell.execute_reply.started":"2025-04-01T10:48:43.720222Z","shell.execute_reply":"2025-04-01T10:48:43.726555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Prepare the test dataset and DataLoader (with is_test=True)\ntest_dataset = MemeDataset(\n    dataframe=test_df,  # 'test_df' contains the test data\n    image_dir=\"/kaggle/input/mesogyny-train-dev-test/test/test images\",  # Correct path to test images\n    tokenizer=tokenizer,\n    transform=image_transform,\n    is_test=True  # Flag to handle test dataset without labels\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Prepare the submission list\nsubmission = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Inference on test data\nwith torch.no_grad():  # Disable gradient computation for inference\n    for i, batch in enumerate(test_loader):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        # Get model predictions\n        logits = model(images, input_ids, attention_mask)\n\n        # Apply sigmoid to get probabilities, then threshold at 0.5 for binary classification\n        preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n        \n        # Store predictions with numerical ID (starting from 1)\n        for j, pred in enumerate(preds):\n            global_index = i * test_loader.batch_size + j + 1  # Starting id from 1\n            submission.append([global_index, int(pred)])  # Storing as (id, prediction)\n\n# Convert the submission list to a DataFrame\nsubmission_df = pd.DataFrame(submission, columns=[\"id\", \"predictions\"])\n\n# Save the submission DataFrame to CSV (without header and index)\nsubmission_df.to_csv('submissions_misogyny1.csv', index=False, header=False)\n\n# Print the first few rows of the submission to verify\nprint(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:48:43.728560Z","iopub.execute_input":"2025-04-01T10:48:43.728816Z","iopub.status.idle":"2025-04-01T10:48:51.330790Z","shell.execute_reply.started":"2025-04-01T10:48:43.728796Z","shell.execute_reply":"2025-04-01T10:48:51.329941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(submission_df.head(50))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:48:51.331899Z","iopub.execute_input":"2025-04-01T10:48:51.332249Z","iopub.status.idle":"2025-04-01T10:48:51.338279Z","shell.execute_reply.started":"2025-04-01T10:48:51.332219Z","shell.execute_reply":"2025-04-01T10:48:51.337342Z"}},"outputs":[],"execution_count":null}]}